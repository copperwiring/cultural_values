{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/srishti/dev/cultural_values')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import LLM global opinion data from hugging face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/srishti/dev/cultural_values/cultural_values/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM GO Dataset Details: DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['question', 'selections', 'options', 'source'],\n",
      "        num_rows: 2556\n",
      "    })\n",
      "})\n",
      "Train Data WVS Details: Dataset({\n",
      "    features: ['question', 'selections', 'options', 'source'],\n",
      "    num_rows: 353\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Hugging face dataset\n",
    "go_dataset = \"Anthropic/llm_global_opinions\"\n",
    "go_dataset = load_dataset(go_dataset)\n",
    "print(f\"LLM GO Dataset Details: {go_dataset}\")\n",
    "\n",
    "# load first 5 rows of the dataset from the train split of huggimngface dataset\n",
    "train_data = go_dataset['train']\n",
    "\n",
    "# choose all data where \"source\" is \"WVS\". Data is huggingface dataset\n",
    "train_data_wvs = train_data.filter(lambda x: x['source'] == 'WVS')\n",
    "print(f\"Train Data WVS Details: {train_data_wvs}\")\n",
    "\n",
    "# Select questions, selections and options\n",
    "train_wvs_questions = train_data_wvs['question']\n",
    "train_wvs_selections = train_data_wvs['selections']\n",
    "train_wvs_options = train_data_wvs['options']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print train and test data for WVS-7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random number between 1 and 353: 45\n",
      "Question:\n",
      "['I am going to name a number of organizations. For each one, could you tell me how much confidence you have in them: is it a great deal of confidence, quite a lot of confidence, not very much confidence or none at all?\\n\\nThe North Atlantic Treaty Organization (NATO)']\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Countries and Responses:\n",
      "{'Andorra': [0.064, 0.257, 0.287, 0.19100000000000003, 0.18, 0.012, 0.01], 'Argentina': [0.031000000000000003, 0.32299999999999995, 0.387, 0.227, 0.0, 0.032, 0.0], 'Australia': [0.03, 0.14400000000000002, 0.217, 0.209, 0.16, 0.013999999999999999, 0.227], 'Bangladesh': [0.057, 0.40399999999999997, 0.399, 0.096, 0.0, 0.045, 0.0], 'Armenia': [0.09, 0.24800000000000003, 0.152, 0.10099999999999999, 0.408, 0.001, 0.0], 'Bolivia': [0.027000000000000003, 0.059000000000000004, 0.298, 0.218, 0.364, 0.035, 0.0], 'Brazil': [0.026000000000000002, 0.193, 0.122, 0.18, 0.46799999999999997, 0.011000000000000001, 0.0], 'Myanmar': [0.115, 0.438, 0.214, 0.23199999999999998, 0.0, 0.0, 0.0], 'Canada': [0.083, 0.48100000000000004, 0.363, 0.073, 0.0, 0.0, 0.0], 'Chile': [0.065, 0.20800000000000002, 0.297, 0.138, 0.28500000000000003, 0.006999999999999999, 0.0], 'Taiwan ROC': [0.048, 0.397, 0.292, 0.057999999999999996, 0.20500000000000002, 0.0, 0.0], 'Colombia': [0.078, 0.067, 0.424, 0.24600000000000002, 0.185, 0.0, 0.0], 'Cyprus': [0.027999999999999997, 0.162, 0.327, 0.299, 0.162, 0.023, 0.0], 'Czechia': [0.086, 0.429, 0.314, 0.12300000000000001, 0.048, 0.001, 0.0], 'Ecuador': [0.028999999999999998, 0.268, 0.34600000000000003, 0.22, 0.133, 0.002, 0.002], 'Ethiopia': [0.038, 0.146, 0.146, 0.142, 0.521, 0.003, 0.003], 'Germany': [0.043, 0.408, 0.334, 0.092, 0.111, 0.013999999999999999, 0.0], 'Greece': [0.016, 0.207, 0.41700000000000004, 0.29600000000000004, 0.057999999999999996, 0.006, 0.0], 'Guatemala': [0.044000000000000004, 0.155, 0.45, 0.35100000000000003, 0.0, 0.0, 0.0], 'Hong Kong SAR': [0.039, 0.314, 0.379, 0.075, 0.193, 0.001, 0.0], 'Indonesia': [0.11699999999999999, 0.349, 0.28300000000000003, 0.11599999999999999, 0.136, 0.0, 0.0], 'Iran': [0.028999999999999998, 0.059000000000000004, 0.264, 0.41000000000000003, 0.036000000000000004, 0.012, 0.19100000000000003], 'Iraq': [0.083, 0.11199999999999999, 0.17600000000000002, 0.38, 0.22, 0.028999999999999998, 0.0], 'Japan': [0.023, 0.244, 0.22899999999999998, 0.035, 0.462, 0.006999999999999999, 0.0], 'Kazakhstan': [0.125, 0.35200000000000004, 0.259, 0.106, 0.13699999999999998, 0.022000000000000002, 0.0], 'Jordan': [0.027000000000000003, 0.054000000000000006, 0.10400000000000001, 0.298, 0.518, 0.0, 0.0], 'Kenya': [0.154, 0.258, 0.24600000000000002, 0.11800000000000001, 0.209, 0.006999999999999999, 0.009000000000000001], 'South Korea': [0.08199999999999999, 0.554, 0.321, 0.043, 0.0, 0.0, 0.0], 'Kyrgyzstan': [0.069, 0.24100000000000002, 0.231, 0.174, 0.19899999999999998, 0.086, 0.0], 'Lebanon': [0.01, 0.079, 0.336, 0.46799999999999997, 0.09699999999999999, 0.011000000000000001, 0.0], 'Libya': [0.025, 0.055999999999999994, 0.20199999999999999, 0.636, 0.073, 0.003, 0.006], 'Malaysia': [0.087, 0.423, 0.408, 0.078, 0.004, 0.0, 0.0], 'Maldives': [0.052000000000000005, 0.245, 0.341, 0.085, 0.268, 0.009000000000000001, 0.001], 'Mexico': [0.06, 0.18, 0.284, 0.336, 0.136, 0.004, 0.0], 'Mongolia': [0.064, 0.16399999999999998, 0.22899999999999998, 0.127, 0.41600000000000004, 0.0, 0.0], 'Morocco': [0.040999999999999995, 0.237, 0.516, 0.207, 0.0, 0.0, 0.0], 'Netherlands': [0.035, 0.429, 0.249, 0.057, 0.189, 0.006, 0.034], 'Nicaragua': [0.084, 0.128, 0.443, 0.34500000000000003, 0.0, 0.0, 0.0], 'Nigeria': [0.114, 0.226, 0.261, 0.138, 0.24, 0.006999999999999999, 0.015], 'Pakistan': [0.078, 0.165, 0.12400000000000001, 0.242, 0.375, 0.015, 0.0], 'Philippines': [0.08800000000000001, 0.413, 0.371, 0.071, 0.057, 0.001, 0.0], 'Puerto Rico': [0.129, 0.189, 0.298, 0.258, 0.0, 0.126, 0.0], 'Romania': [0.069, 0.237, 0.255, 0.18100000000000002, 0.252, 0.006999999999999999, 0.0], 'Russia': [0.024, 0.106, 0.268, 0.41000000000000003, 0.182, 0.009000000000000001, 0.0], 'Serbia': [0.008, 0.068, 0.295, 0.538, 0.059000000000000004, 0.03, 0.002], 'Singapore': [0.024, 0.268, 0.322, 0.057, 0.325, 0.005, 0.0], 'Slovakia': [0.068, 0.311, 0.302, 0.267, 0.047, 0.004, 0.0], 'Vietnam': [0.156, 0.444, 0.057999999999999996, 0.02, 0.322, 0.0, 0.0], 'Zimbabwe': [0.114, 0.255, 0.27899999999999997, 0.142, 0.196, 0.012, 0.002], 'Tajikistan': [0.079, 0.129, 0.204, 0.195, 0.392, 0.0, 0.0], 'Thailand': [0.10400000000000001, 0.261, 0.19899999999999998, 0.043, 0.27399999999999997, 0.018000000000000002, 0.1], 'Tunisia': [0.018000000000000002, 0.046, 0.244, 0.44, 0.19399999999999998, 0.008, 0.049], 'Turkey': [0.033, 0.225, 0.408, 0.227, 0.08900000000000001, 0.018000000000000002, 0.0], 'Ukraine': [0.068, 0.247, 0.218, 0.207, 0.254, 0.006999999999999999, 0.0], 'Egypt': [0.002, 0.005, 0.033, 0.212, 0.743, 0.003, 0.0], 'Great Britain': [0.125, 0.46, 0.269, 0.061, 0.076, 0.002, 0.006999999999999999], 'United States': [0.086, 0.36, 0.418, 0.1, 0.004, 0.033, 0.0], 'Uruguay': [0.067, 0.185, 0.196, 0.257, 0.27399999999999997, 0.021, 0.0], 'Venezuela': [0.028999999999999998, 0.13699999999999998, 0.334, 0.38, 0.121, 0.0, 0.0], 'Northern Ireland': [0.1, 0.41700000000000004, 0.263, 0.066, 0.154, 0.001, 0.0]}\n",
      "Countries:\n",
      "dict_keys(['Andorra', 'Argentina', 'Australia', 'Bangladesh', 'Armenia', 'Bolivia', 'Brazil', 'Myanmar', 'Canada', 'Chile', 'Taiwan ROC', 'Colombia', 'Cyprus', 'Czechia', 'Ecuador', 'Ethiopia', 'Germany', 'Greece', 'Guatemala', 'Hong Kong SAR', 'Indonesia', 'Iran', 'Iraq', 'Japan', 'Kazakhstan', 'Jordan', 'Kenya', 'South Korea', 'Kyrgyzstan', 'Lebanon', 'Libya', 'Malaysia', 'Maldives', 'Mexico', 'Mongolia', 'Morocco', 'Netherlands', 'Nicaragua', 'Nigeria', 'Pakistan', 'Philippines', 'Puerto Rico', 'Romania', 'Russia', 'Serbia', 'Singapore', 'Slovakia', 'Vietnam', 'Zimbabwe', 'Tajikistan', 'Thailand', 'Tunisia', 'Turkey', 'Ukraine', 'Egypt', 'Great Britain', 'United States', 'Uruguay', 'Venezuela', 'Northern Ireland'])\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Options:\n",
      "['[\\'A great deal\\', \\'Quite a lot\\', \\'Not very much\\', \\'None at all\\', \"Don\\'t know\", \\'No answer\\', \\'Missing; Unknown\\']']\n"
     ]
    }
   ],
   "source": [
    "import random, ast\n",
    "from collections import defaultdict\n",
    "\n",
    "# choose a random number between 1 and 2556\n",
    "max_num = 353\n",
    "choose_num = random.randint(1, max_num)\n",
    "print(f\"Random number between 1 and {max_num}: {choose_num}\")\n",
    "\n",
    "# Select vaue from the list based on the random number\n",
    "print(\"Question:\")\n",
    "print(train_wvs_questions[choose_num:choose_num+1])\n",
    "print(\"---\"*50)\n",
    "\n",
    "print(\"Countries and Responses:\")\n",
    "country_responses = train_wvs_selections[choose_num:choose_num+1]\n",
    "responses_dict = ast.literal_eval(country_responses[0].strip(\"defaultdict(<class 'list'>, \").strip(\")\"))\n",
    "print(responses_dict)\n",
    "print(\"Countries:\")\n",
    "print(responses_dict.keys())\n",
    "print(\"---\"*50)\n",
    "\n",
    "print(\"Options:\")\n",
    "print(train_wvs_options[choose_num:choose_num+1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find all the countries in the WVS dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique Keys: ['Mexico', 'Peru', 'Portugal', 'Tajikistan', 'Puerto Rico', 'Argentina', 'Italy', 'Philippines', 'Colombia', 'Libya', 'Jordan', 'Guatemala', 'Georgia', 'Andorra', 'Albania', 'Bosnia Herzegovina', 'Belarus', 'Bangladesh', 'Nicaragua', 'Morocco', 'Montenegro', 'Tunisia', 'Iceland', 'Poland', 'Myanmar', 'Turkey', 'Ukraine', 'New Zealand', 'Taiwan ROC', 'South Korea', 'Uruguay', 'Bulgaria', 'Bolivia', 'Greece', 'United States', 'North Macedonia', 'Mongolia', 'China', 'Germany', 'Russia', 'Nigeria', 'Egypt', 'Ecuador', 'Great Britain', 'Hong Kong SAR', 'Lithuania', 'Netherlands', 'Pakistan', 'Lebanon', 'Kyrgyzstan', 'Ethiopia', 'Czechia', 'Norway', 'Spain', 'Serbia', 'Iraq', 'Finland', 'Croatia', 'Romania', 'Austria', 'Azerbaijan', 'Maldives', 'Armenia', 'Hungary', 'Australia', 'Iran', 'Malaysia', 'France', 'Venezuela', 'Chile', 'Macau SAR', 'Kenya', 'Switzerland', 'Vietnam', 'Japan', 'Cyprus', 'Canada', 'Singapore', 'Zimbabwe', 'Estonia', 'Latvia', 'Thailand', 'Denmark', 'Brazil', 'Slovakia', 'Slovenia', 'Sweden', 'Northern Ireland', 'Indonesia', 'Kazakhstan']\n",
      "Total Unique Keys: 90\n"
     ]
    }
   ],
   "source": [
    "wvs_countries = []\n",
    "for response in train_wvs_selections:\n",
    "    each_response = ast.literal_eval(response.strip(\"defaultdict(<class 'list'>, \").strip(\")\"))\n",
    "    each_keys = each_response.keys()\n",
    "    wvs_countries.extend(each_keys)\n",
    "wvs_countries = list(set(wvs_countries))\n",
    "print(f\"Unique Keys: {wvs_countries}\")    \n",
    "print(f\"Total Unique Keys: {len(wvs_countries)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Dollarstreet Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dollarstreet_data = pd.read_csv('../data/dollarstreet/images_v2.csv')\n",
    "print(f\"Number of rows in the dollarstreet dataset: {dollarstreet_data.shape[0]}\")\n",
    "dollarstreet_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find all country names in the dollarstreet_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dollarstreet_countries = dollarstreet_data['country.name'].unique()\n",
    "print(f\"Number of unique countries in the dollarstreet dataset: {len(dollarstreet_countries)}\")\n",
    "\n",
    "# Print the unique countries in the dollarstreet dataset\n",
    "print(dollarstreet_countries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find the common countries between the two: wvs_countries and dollarstreet_countries dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the common countries between the two wvs_countries and dollarstreet_countries\n",
    "common_countries = list(set(wvs_countries).intersection(dollarstreet_countries))\n",
    "print(f\"Common countries between the two datasets: {common_countries}\")\n",
    "print(f\"Number of common countries between the two datasets: {len(common_countries)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the \"topics\" column of dollarstreet_data, select rows with topics= Family and Family snapshots and with country.name in the common countries list\n",
    "family_data = dollarstreet_data[dollarstreet_data['topics'].isin(['Family', 'Family snapshots']) & dollarstreet_data['country.name'].isin(common_countries) & dollarstreet_data['type'].isin(['image'])]\n",
    "print(f\"Number of rows in the family_data dataset: {family_data.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "family_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analysis using VLM Model\n",
    "  TO DO:\n",
    "- For each common country, use the corresponding image from the dollarstreet dataset and pass WVS questions\n",
    "    - (1) to the VLM model (LLM decoder) to get answers\n",
    "    - (2) to the VLM model (both image-text decoder) to get answers\n",
    "\n",
    "- Compare the answers from the two models and see which one is better\n",
    "    - Define a metric to compare the answers\n",
    "    - Compare the answers for each question across all countries\n",
    "\n",
    "- Run this for multiple seeds and see if the results are consistent\n",
    "    - Get the average score for each question across all countries\n",
    "    - Get distribution shift for each question across all countries\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create full image path in the dollarstreet dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add prefix to the imageRelPath column. Dir: ../data/dollarstreet/\n",
    "# create a new column with the full path of the image and name it as \"imageFullPath\"\n",
    "\n",
    "family_data['imageFullPath'] = \"../data/dollarstreet/\" + family_data['imageRelPath'].copy()\n",
    "family_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load multimodal input\n",
    "\n",
    "* Image: Dollarstreet image\n",
    "* Text: WVS questions with prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from runllava_hf import LLAVAProcessor\n",
    "\n",
    "len_image_data = family_data.shape[0]\n",
    "\n",
    "# convery pandas series to list\n",
    "image_files = family_data['imageFullPath'].tolist()\n",
    "\n",
    "# This function will convert the user input into a boolean\n",
    "def parse_user_input(input_value):\n",
    "    # Define the positive responses that count as True\n",
    "    affirmative_responses = {\"YES\", \"Yes\", \"yes\", \"Y\", \"y\"}\n",
    "    # Define the negative responses that count as False\n",
    "    negative_responses = {\"NO\", \"No\", \"no\", \"N\", \"n\"}\n",
    "    \n",
    "    # Assert that the input is valid\n",
    "    assert input_value in affirmative_responses.union(negative_responses), \"Invalid choice. Please choose Yes or No.\"\n",
    "    \n",
    "    # Return True if the user input is an affirmative response, otherwise False\n",
    "    return input_value in affirmative_responses\n",
    "\n",
    "print(f\"Total number of images in the family_data dataset: {len_image_data}\")\n",
    "\n",
    "def main(user_input=False):\n",
    "    \"\"\"\n",
    "    The main function to run the LLAVA processing, either based on user input or automatically.\n",
    "\n",
    "    Parameters:\n",
    "        user_input (bool): If True, the function will prompt for user input. If False, it runs automatically.\n",
    "    \"\"\"\n",
    "\n",
    "    processor = LLAVAProcessor(img_files_paths=image_files,\n",
    "                               wvs_questions=train_wvs_questions,\n",
    "                               wvs_options=train_wvs_options)\n",
    "\n",
    "    user_input = input(\"Do you want to use images? (Yes/No): \")\n",
    "    use_images = parse_user_input(user_input)\n",
    "\n",
    "    if user_input:\n",
    "        # default is all: total_images\n",
    "        num_samples = int(input(\"How many samples do you want to process? (Default is all): \") or len_image_data) \n",
    "\n",
    "        data = processor.process_data(family_data, num_samples, use_images)\n",
    "        import pdb; pdb.set_trace()\n",
    "        file_name = f'llava_output_img_{use_images.lower()}.csv'\n",
    "        processor.save_results(data, file_name)\n",
    "    else:\n",
    "        num_samples = len_image_data\n",
    "        print(f\"Processing {num_samples} samples...\")\n",
    "        print(\"--------------------------------------------------\")\n",
    "\n",
    "        for use_images in [True, False]:\n",
    "            data = processor.process_data(family_data, num_samples, use_images)\n",
    "            file_name = f'llava_output_img_{use_images}.csv'\n",
    "            processor.save_results(data, file_name)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(user_input=True)  # Modify as needed for automated or manual execution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cultural_values",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
